{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of GPT-2 conversation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLXW02eIYpcB",
        "colab_type": "text"
      },
      "source": [
        "clone and cd into repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICYu3w9hIJkC",
        "colab_type": "code",
        "outputId": "00f8a590-330e-4f9a-8375-c09f786752da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# first download the gpt-2 code\n",
        "!git clone https://github.com/nshepperd/gpt-2.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'gpt-2' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBvmbwX7ePev",
        "colab_type": "code",
        "outputId": "c33a3271-bcc0-460e-e69a-0f4f7c22b895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eEIs3ApZUVO",
        "colab_type": "code",
        "outputId": "e8cdbef4-1d2e-4dee-dd11-c09dbea62170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd gpt-2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtn1qZPgZLb0",
        "colab_type": "text"
      },
      "source": [
        "install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "434oOx0bZH6J",
        "colab_type": "code",
        "outputId": "cdd076c7-679d-4796-fe36-1d9393f6b72d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fire>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.2.1)\n",
            "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2017.4.5)\n",
            "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Requirement already satisfied: tqdm==4.31.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.31.1)\n",
            "Requirement already satisfied: toposort==1.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1hrgeKFYsuE",
        "colab_type": "text"
      },
      "source": [
        "download the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQUZAa9cIcTY",
        "colab_type": "code",
        "outputId": "8c03c107-ab59-4bcf-a59a-0228280dd0b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# download the pretrained model\n",
        "!python download_model.py 345M"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 1.14Mit/s]                                                     \n",
            "Fetching encoder.json: 1.04Mit [00:00, 50.9Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.18Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:17, 82.2Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 7.80Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 67.3Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 58.5Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oJPQtdLbbeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrV4qjoWc8ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUw5FEn-fIqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/gpt-2/src\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgs6ekD1fItO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "ff77cbd7-e8d4-4e89-dbdd-81746145380f"
      },
      "source": [
        "import fire\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import model, sample, encoder"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Hqb_vDxfMza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import generate_unconditional_samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRMB0HAjhC2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import interactive_conditional_samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvkAIuq5fb3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GPT2:\n",
        "\n",
        "  \n",
        "  # extracted from the source code to generate some text based on a prior\n",
        "  def __init__(\n",
        "      self,\n",
        "      model_name='345M',\n",
        "      seed=None,\n",
        "      nsamples=1,\n",
        "      batch_size=1,\n",
        "      length=None,\n",
        "      temperature=1,\n",
        "      top_k=0,\n",
        "      raw_text=\"\",\n",
        "  ):\n",
        "      \"\"\"\n",
        "      Interactively run the model\n",
        "      :model_name=117M : String, which model to use\n",
        "      :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
        "       results\n",
        "      :nsamples=1 : Number of samples to return total\n",
        "      :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
        "      :length=None : Number of tokens in generated text, if None (default), is\n",
        "       determined by model hyperparameters\n",
        "      :temperature=1 : Float value controlling randomness in boltzmann\n",
        "       distribution. Lower temperature results in less random completions. As the\n",
        "       temperature approaches zero, the model will become deterministic and\n",
        "       repetitive. Higher temperature results in more random completions.\n",
        "      :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
        "       considered for each step (token), resulting in deterministic completions,\n",
        "       while 40 means 40 words are considered at each step. 0 (default) is a\n",
        "       special setting meaning no restrictions. 40 generally is a good value.\n",
        "      \"\"\"\n",
        "      if batch_size is None:\n",
        "          batch_size = 1\n",
        "      assert nsamples % batch_size == 0\n",
        "\n",
        "      self.nsamples = nsamples\n",
        "      self.batch_size = batch_size\n",
        "      \n",
        "      self.enc = encoder.get_encoder(model_name)\n",
        "      hparams = model.default_hparams()\n",
        "      with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
        "          hparams.override_from_dict(json.load(f))\n",
        "\n",
        "      if length is None:\n",
        "          length = hparams.n_ctx // 2\n",
        "      elif length > hparams.n_ctx:\n",
        "          raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "      self.sess = tf.Session(graph=tf.Graph())\n",
        "      self.sess.__enter__()\n",
        "      \n",
        "      self.context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "      np.random.seed(seed)\n",
        "      tf.set_random_seed(seed)\n",
        "      self.output = sample.sample_sequence(\n",
        "          hparams=hparams, length=length,\n",
        "          context=self.context,\n",
        "          batch_size=batch_size,\n",
        "          temperature=temperature, top_k=top_k\n",
        "      )\n",
        "\n",
        "      saver = tf.train.Saver()\n",
        "      self.ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name))\n",
        "      saver.restore(self.sess, self.ckpt)\n",
        "\n",
        "  def close(self):\n",
        "    self.sess.close()\n",
        "  \n",
        "  def generate_conditional(self,raw_text):\n",
        "      context_tokens = self.enc.encode(raw_text)\n",
        "      generated = 0\n",
        "      for _ in range(self.nsamples // self.batch_size):\n",
        "          out = self.sess.run(self.output, feed_dict={\n",
        "              self.context: [context_tokens for _ in range(self.batch_size)]\n",
        "          })[:, len(context_tokens):]\n",
        "          for i in range(self.batch_size):\n",
        "              generated += 1\n",
        "              text = self.enc.decode(out[i])\n",
        "              return text\n",
        "              #print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "              #print(text)\n",
        "      #print(\"=\" * 80)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoJbf02UOFed",
        "colab_type": "code",
        "outputId": "b8255740-e9d3-400b-fca3-5a4da9df4e07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "gpt2 = GPT2()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from models/345M/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAdyhE0JgFV6",
        "colab_type": "code",
        "outputId": "5890f951-2a0f-4af9-b978-59552499d47a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "result = gpt2.generate_conditional(raw_text=\"Can you tell me something about music?\")\n",
        "\n",
        "print(result)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "SW: Well, I'm an rapping comedian now and the thing I like most about every guy who follows me on Twitter is that whenever I'm stuck in the moment, I'm out there freestyling or tinkering with whatever freakin' notepaper they sold at Twiztid. My favorite example: Two years ago, I tweeted \"You people make my fans look like Commie dogs!\" I had actually never seen that tweet, and had a hard time telling the difference between the two platforms. My Twitter handle was @itwangoCause… but through those two tweets (which I posted over the course of a few months), I was able to stream over a thousand Twitter followers while simultaneously spreading pure KOREA Weibo envy fablities across the nation.\n",
            "\n",
            "Since then, I've become an expert on KOREA themes, and have been engaged in a frenzy of blasting myself here and there. All of it made for a thoroughly schmaltzy stream of hilarity, even more efficient than poking fun at myself on Twitter. Occasionally though, it's hard to time your digital attacks enough simply to show up in time for the live broadcast… Unless they magically work like mosquitoes. Then, of course… You can see it all live, live. Just wake up, chill out, and watch.\n",
            "\n",
            "Now, just to be clear, my hatred toward US AFGHANISTAN DOES NOT COME FROM ANY POSITION OF...\n",
            "\n",
            "SW: All right, good snag. For the uninitiated, athletic training is, the tradition goes like this:\n",
            "\n",
            "7:45 AM: Proper stance are counted. Being unshod takes more weight and foundation than an untrained person, but it comes naturally and has its benefits The lower posture causes the hands to joint widening, weakening your traps As well, about secondarily from entering the 'flow', the % of nerve stores between the BT & the midrange region, the previous put-down and shift to running, fatigue and wider shoulders, the final sub-threshold builds\n",
            "\n",
            "So yeah, I realized that everybody has very similar broken form when I'm wrestling at 6:30 am in Boston when identical percentages of nerve stores, tendons and muscles inside and around biceps, triceps, shoulder, back and hips are rotting away along with your core joints with severe attachment issues.\n",
            "\n",
            "SW: I guess that is what I was to most insulting commenters. When I proudly claim \"I am, without a doubt, the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erXh3GvcP1Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Who:\n",
        "  \"\"\"A class defining the conversation parties: me, he\"\"\"\n",
        "  def __init__(self):\n",
        "    self.prefixes = []\n",
        "\n",
        "  def matches(self,phrase):\n",
        "    for prefix in self.prefixes:\n",
        "      if phrase.startswith(prefix):\n",
        "        #print(f\"{phrase} starts with {prefix}\")\n",
        "        return True\n",
        "      \n",
        "    #print(f\"{phrase} does not start with {self.prefixes}\")\n",
        "    return False\n",
        "\n",
        "  def get_random_prefix(self):\n",
        "    return self.prefixes[0]\n",
        "  \n",
        "class Me(Who):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.prefixes = [\"I said: \\\"\"]\n",
        "   \n",
        "  \n",
        "class You(Who):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.prefixes = [\"You said: \\\"\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjS58QZlrVKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Conversation:\n",
        "  \n",
        "  def __init__(self, prior = None):\n",
        "    if prior is None:\n",
        "      prior=\"\"\"\n",
        "      You said: \"Nice to meet you. What's your name?\"\n",
        "      I said: \"My name is Pete.\"\n",
        "      You said: \"That's an interesting name. How old are you?\"\n",
        "      I said: \"I'm 40 years old.\"\n",
        "      You said: \"Can you tell me something about yourself?\"\n",
        "      I said: \"Ofcourse! I like playing video games and eating cake. \"\n",
        "      You said: \"I like sweet stuff too. What are your plans for tomorrow?\"\n",
        "      \"\"\"\n",
        "    self.suggestion = None\n",
        "    \n",
        "    self.me = Me()\n",
        "    self.you = You()\n",
        "    self.parties  = [ self.me, self.you ]\n",
        "    \n",
        "    self.conversation = []\n",
        "    \n",
        "    lines = prior.split(\"\\n\")\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if len(line)!=0:\n",
        "        party = None\n",
        "        for party in self.parties:\n",
        "          if party.matches(line):\n",
        "            break\n",
        "        if party is None:\n",
        "          raise Exception(f\"Unknown party: {line}\")\n",
        "                \n",
        "        self.conversation.append((party,line))\n",
        "    self.get_suggestion()\n",
        "    \n",
        "  \n",
        "  def get_prior(self):\n",
        "    conv = \"\"\n",
        "    for (party, line) in self.conversation:\n",
        "      conv+=line+\"\\n\"\n",
        "    return conv\n",
        "  \n",
        "  def get_suggestion(self):\n",
        "    who, last_line = self.conversation[-1]\n",
        "\n",
        "    party_index = self.parties.index(who)\n",
        "    next_party = self.parties[(party_index+1) % len(self.parties)]\n",
        "      \n",
        "    conv = self.get_prior()\n",
        "    conv += next_party.get_random_prefix()\n",
        "    answer = self.get_answer(next_party, conv)\n",
        "\n",
        "    if not next_party.matches(answer):\n",
        "      prefix = next_party.get_random_prefix()\n",
        "      answer = prefix + answer\n",
        "    \n",
        "    self.suggestion = (next_party, answer)\n",
        "  \n",
        "  def next(self, party = None, answer = \"\"):\n",
        "    \"\"\"Continue the conversation\n",
        "    :param party: None -> use the current party which is currently in turn\n",
        "    :param answer: None -> use the suggestion, specify a text to override the \n",
        "           suggestion\n",
        "    \n",
        "    \"\"\"\n",
        "    suggested_party, suggested_answer = self.suggestion\n",
        "    if party is None:\n",
        "      party = suggested_party\n",
        "    \n",
        "    if answer == \"\":\n",
        "      answer = suggested_answer\n",
        "      \n",
        "    if not party.matches(answer):\n",
        "      prefix = party.get_random_prefix()\n",
        "      answer = prefix + answer\n",
        "    \n",
        "    answer = answer.strip()\n",
        "    if answer[-1] != \"\\\"\":\n",
        "      # add the closing \"\n",
        "      answer += \"\\\"\"\n",
        "      \n",
        "    self.conversation.append((party, answer))    \n",
        "    self.get_suggestion()\n",
        "    \n",
        "  def retry(self):\n",
        "    self.get_suggestion()\n",
        "        \n",
        "  def get_answer(self, party, conv):\n",
        "    answer = gpt2.generate_conditional(raw_text=conv)\n",
        "    lines = answer.split(\"\\n\")\n",
        "    line = \"\"\n",
        "    for line in lines:\n",
        "      if line !=\"\":\n",
        "        break\n",
        "      \n",
        "    if line!=\"\":\n",
        "      return line\n",
        "    \n",
        "    return \"\"\n",
        "      \n",
        "  def show(self):\n",
        "    conv = \"\"\n",
        "    for (party, line) in self.conversation:\n",
        "      conv+=line+\"\\n\"\n",
        "    print(conv)\n",
        "    if self.suggestion is not None:\n",
        "      party, answer  = self.suggestion\n",
        "      print(\"--> \"+answer)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GYnMZDPgr7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = Conversation(\"\"\"\n",
        "Annie said: hello. my name is annie. what's your name?\n",
        "Bottie said: hi. my name is bottie. \n",
        "Annie said: so nice to meet you!\n",
        "Bottie said: you too!\n",
        "Annie said: how old are you?\n",
        "Bottie said: i'm 20 years old.\n",
        "Annie said: nice! i'm 21 years old. i am in college right now.\n",
        "Bottie said: what's your favorite class?\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taqx6zfNVbru",
        "colab_type": "code",
        "outputId": "b8edb905-d29d-4892-c584-57bbaf4d5552",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "# show the conversation and the suggestion by the ai\n",
        "c.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Annie said: hello. my name is annie. what's your name?\n",
            "Bottie said: hi. my name is bottie.\n",
            "Annie said: so nice to meet you!\n",
            "Bottie said: you too!\n",
            "Annie said: how old are you?\n",
            "Bottie said: i'm 20 years old.\n",
            "Annie said: nice! i'm 21 years old. i am in college right now.\n",
            "Bottie said: what's your favorite class?\n",
            "\n",
            "--> I said: \"Act I.\" I said that when i was 19 years old. But i never asked because i didn't want to hurt my voice, and i have carried my ass in rehearsals since then. I love to act on stage.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUICPSgdfcp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"I said\" -> answer by the AI\n",
        "# if the answer of the AI is garbage then call c.retry() \n",
        "c.retry()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SvC-10-WTKH",
        "colab_type": "code",
        "outputId": "dc5b8415-b8d8-417c-b02c-d2408e54336c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "c.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Annie said: hello. my name is annie. what's your name?\n",
            "Bottie said: hi. my name is bottie.\n",
            "Annie said: so nice to meet you!\n",
            "Bottie said: you too!\n",
            "Annie said: how old are you?\n",
            "Bottie said: i'm 20 years old.\n",
            "Annie said: nice! i'm 21 years old. i am in college right now.\n",
            "Bottie said: what's your favorite class?\n",
            "\n",
            "--> I said: \"biology\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNsuyeKyeNeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Accept the suggested answer by the AI\n",
        "c.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ8F3HbyUDZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# now its your turn\n",
        "c.next(c.you, \"My favorite pizza is the calzone with lots of cheese.\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ts3yigFeQ-8",
        "colab_type": "code",
        "outputId": "d135363b-66e6-406d-e055-386542c1b9ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "c.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You said: \"Nice to meet you. What's your name?\"\n",
            "I said: \"My name is Pete.\"\n",
            "You said: \"That's an interesting name. How old are you?\"\n",
            "I said: \"I'm 40 years old.\"\n",
            "You said: \"Can you tell me something about yourself?\"\n",
            "I said: \"Ofcourse! I like playing video games and eating cake. \"\n",
            "You said: \"I like sweet stuff too. What are your plans for tomorrow?\"\n",
            "I said: \"Nothing, and I don't know what to do with my life! \"\n",
            "\n",
            "--> You said: \"Do you like following the advice of experts? Or do you have all your own ideas?\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR5UP1WQXCi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUiQqYdEhmwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now its your turn\n",
        "c.next(c.you, \"You eat pizza often?\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snnqjfilc1Ai",
        "colab_type": "code",
        "outputId": "d4447020-8cb1-44be-abe1-83390cef3a11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "# show the conversation and the reply of the ai\n",
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You said: \"Nice to meet you. What's your name?\"\n",
            "I said: \"My name is Pete.\"\n",
            "You said: \"That's an interesting name. How old are you?\"\n",
            "I said: \"I'm 40 years old.\"\n",
            "You said: \"Can you tell me something about yourself?\"\n",
            "I said: \"Ofcourse! I like playing video games and eating cake. \"\n",
            "You said: \"I like sweet stuff too. What are your plans for tomorrow?\"\n",
            "I said: \"I will participate in playing video games and try to eaten pizza, I'm going to want some apples 'cause I kiss big apples...ill be eating them.\"\n",
            "You said: \"My favorite pizza is the calzone with lots of cheese.\"\n",
            "I said: \"Sounds good.\"\n",
            "You said: \"You eat pizza often?\"\n",
            "\n",
            "--> I said: \"Yes, I do. I would like to eat more often...and rock my rocking armor.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R6da2YMS6fD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accept ai answer\n",
        "c.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPJZ_aJxltCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# our reply\n",
        "c.next(c.you, \"Pizza is not to good for your health though.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e66v5ew6sQW4",
        "colab_type": "code",
        "outputId": "921d7494-68b1-4e19-cf89-a1904349aee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "# show the conv. and reply of the ai\n",
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You said: \"Nice to meet you. What's your name?\"\n",
            "I said: \"My name is Pete.\"\n",
            "You said: \"That's an interesting name. How old are you?\"\n",
            "I said: \"I'm 40 years old.\"\n",
            "You said: \"Can you tell me something about yourself?\"\n",
            "I said: \"Ofcourse! I like playing video games and eating cake. \"\n",
            "You said: \"I like sweet stuff too. What are your plans for tomorrow?\"\n",
            "I said: \"I will participate in playing video games and try to eaten pizza, I'm going to want some apples 'cause I kiss big apples...ill be eating them.\"\n",
            "You said: \"My favorite pizza is the calzone with lots of cheese.\"\n",
            "I said: \"Sounds good.\"\n",
            "You said: \"You eat pizza often?\"\n",
            "I said: \"Yes, I do. I would like to eat more often...and rock my rocking armor.\"\n",
            "You said: \"Pizza is not to good for your health though.\"\n",
            "\n",
            "--> I said: \"Oh yeah. I do get so tired from it.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2epsBUCPtcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# end the session\n",
        "gpt2.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}