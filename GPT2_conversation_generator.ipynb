{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLXW02eIYpcB"
   },
   "source": [
    "clone and cd into repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ICYu3w9hIJkC",
    "outputId": "00f8a590-330e-4f9a-8375-c09f786752da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'gpt-2' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "# first download the gpt-2 code\n",
    "!git clone https://github.com/nshepperd/gpt-2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "cBvmbwX7ePev",
    "outputId": "c33a3271-bcc0-460e-e69a-0f4f7c22b895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anniesu/Documents/school/college_4/SEM_1/CIS_519/cis519_final_project\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6eEIs3ApZUVO",
    "outputId": "e8cdbef4-1d2e-4dee-dd11-c09dbea62170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anniesu/Documents/school/college_4/SEM_1/CIS_519/cis519_final_project/gpt-2\n"
     ]
    }
   ],
   "source": [
    "cd gpt-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qtn1qZPgZLb0"
   },
   "source": [
    "install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "434oOx0bZH6J",
    "outputId": "cdd076c7-679d-4796-fe36-1d9393f6b72d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fire>=0.1.3 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (2017.4.5)\n",
      "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
      "Requirement already satisfied: tqdm==4.31.1 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (4.31.1)\n",
      "Requirement already satisfied: toposort==1.5 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (1.5)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/site-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.13.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1hrgeKFYsuE"
   },
   "source": [
    "download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "nQUZAa9cIcTY",
    "outputId": "8c03c107-ab59-4bcf-a59a-0228280dd0b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.00kit [00:00, 173kit/s]                                                      \n",
      "Fetching encoder.json: 1.04Mit [00:00, 9.05Mit/s]                                                   \n",
      "Fetching hparams.json: 1.00kit [00:00, 203kit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 1.42Git [01:29, 15.9Mit/s]                                 \n",
      "Fetching model.ckpt.index: 11.0kit [00:00, 3.83Mit/s]                                               \n",
      "Fetching model.ckpt.meta: 927kit [00:00, 7.81Mit/s]                                                 \n",
      "Fetching vocab.bpe: 457kit [00:00, 4.73Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "# download the pretrained model\n",
    "!python download_model.py 345M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7oJPQtdLbbeK"
   },
   "outputs": [],
   "source": [
    "!export PYTHONIOENCODING=UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrV4qjoWc8ej"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UUw5FEn-fIqW"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/gpt-2/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "Qgs6ekD1fItO",
    "outputId": "ff77cbd7-e8d4-4e89-dbdd-81746145380f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fire'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-be3bfb740a8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fire'"
     ]
    }
   ],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import model, sample, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Hqb_vDxfMza"
   },
   "outputs": [],
   "source": [
    "import generate_unconditional_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iRMB0HAjhC2v"
   },
   "outputs": [],
   "source": [
    "import interactive_conditional_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hvkAIuq5fb3d"
   },
   "outputs": [],
   "source": [
    "class GPT2:\n",
    "\n",
    "  \n",
    "  # extracted from the source code to generate some text based on a prior\n",
    "  def __init__(\n",
    "      self,\n",
    "      model_name='345M',\n",
    "      seed=None,\n",
    "      nsamples=1,\n",
    "      batch_size=1,\n",
    "      length=None,\n",
    "      temperature=1,\n",
    "      top_k=0,\n",
    "      raw_text=\"\",\n",
    "  ):\n",
    "      \"\"\"\n",
    "      Interactively run the model\n",
    "      :model_name=117M : String, which model to use\n",
    "      :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
    "       results\n",
    "      :nsamples=1 : Number of samples to return total\n",
    "      :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
    "      :length=None : Number of tokens in generated text, if None (default), is\n",
    "       determined by model hyperparameters\n",
    "      :temperature=1 : Float value controlling randomness in boltzmann\n",
    "       distribution. Lower temperature results in less random completions. As the\n",
    "       temperature approaches zero, the model will become deterministic and\n",
    "       repetitive. Higher temperature results in more random completions.\n",
    "      :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
    "       considered for each step (token), resulting in deterministic completions,\n",
    "       while 40 means 40 words are considered at each step. 0 (default) is a\n",
    "       special setting meaning no restrictions. 40 generally is a good value.\n",
    "      \"\"\"\n",
    "      if batch_size is None:\n",
    "          batch_size = 1\n",
    "      assert nsamples % batch_size == 0\n",
    "\n",
    "      self.nsamples = nsamples\n",
    "      self.batch_size = batch_size\n",
    "      \n",
    "      self.enc = encoder.get_encoder(model_name)\n",
    "      hparams = model.default_hparams()\n",
    "      with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
    "          hparams.override_from_dict(json.load(f))\n",
    "\n",
    "      if length is None:\n",
    "          length = hparams.n_ctx // 2\n",
    "      elif length > hparams.n_ctx:\n",
    "          raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "      self.sess = tf.Session(graph=tf.Graph())\n",
    "      self.sess.__enter__()\n",
    "      \n",
    "      self.context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "      np.random.seed(seed)\n",
    "      tf.set_random_seed(seed)\n",
    "      self.output = sample.sample_sequence(\n",
    "          hparams=hparams, length=length,\n",
    "          context=self.context,\n",
    "          batch_size=batch_size,\n",
    "          temperature=temperature, top_k=top_k\n",
    "      )\n",
    "\n",
    "      saver = tf.train.Saver()\n",
    "      self.ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name))\n",
    "      saver.restore(self.sess, self.ckpt)\n",
    "\n",
    "  def close(self):\n",
    "    self.sess.close()\n",
    "  \n",
    "  def generate_conditional(self,raw_text):\n",
    "      context_tokens = self.enc.encode(raw_text)\n",
    "      generated = 0\n",
    "      for _ in range(self.nsamples // self.batch_size):\n",
    "          out = self.sess.run(self.output, feed_dict={\n",
    "              self.context: [context_tokens for _ in range(self.batch_size)]\n",
    "          })[:, len(context_tokens):]\n",
    "          for i in range(self.batch_size):\n",
    "              generated += 1\n",
    "              text = self.enc.decode(out[i])\n",
    "              return text\n",
    "              #print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "              #print(text)\n",
    "      #print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CoJbf02UOFed",
    "outputId": "b8255740-e9d3-400b-fca3-5a4da9df4e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/345M/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "gpt2 = GPT2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "id": "oAdyhE0JgFV6",
    "outputId": "5890f951-2a0f-4af9-b978-59552499d47a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "SW: Well, I'm an rapping comedian now and the thing I like most about every guy who follows me on Twitter is that whenever I'm stuck in the moment, I'm out there freestyling or tinkering with whatever freakin' notepaper they sold at Twiztid. My favorite example: Two years ago, I tweeted \"You people make my fans look like Commie dogs!\" I had actually never seen that tweet, and had a hard time telling the difference between the two platforms. My Twitter handle was @itwangoCause… but through those two tweets (which I posted over the course of a few months), I was able to stream over a thousand Twitter followers while simultaneously spreading pure KOREA Weibo envy fablities across the nation.\n",
      "\n",
      "Since then, I've become an expert on KOREA themes, and have been engaged in a frenzy of blasting myself here and there. All of it made for a thoroughly schmaltzy stream of hilarity, even more efficient than poking fun at myself on Twitter. Occasionally though, it's hard to time your digital attacks enough simply to show up in time for the live broadcast… Unless they magically work like mosquitoes. Then, of course… You can see it all live, live. Just wake up, chill out, and watch.\n",
      "\n",
      "Now, just to be clear, my hatred toward US AFGHANISTAN DOES NOT COME FROM ANY POSITION OF...\n",
      "\n",
      "SW: All right, good snag. For the uninitiated, athletic training is, the tradition goes like this:\n",
      "\n",
      "7:45 AM: Proper stance are counted. Being unshod takes more weight and foundation than an untrained person, but it comes naturally and has its benefits The lower posture causes the hands to joint widening, weakening your traps As well, about secondarily from entering the 'flow', the % of nerve stores between the BT & the midrange region, the previous put-down and shift to running, fatigue and wider shoulders, the final sub-threshold builds\n",
      "\n",
      "So yeah, I realized that everybody has very similar broken form when I'm wrestling at 6:30 am in Boston when identical percentages of nerve stores, tendons and muscles inside and around biceps, triceps, shoulder, back and hips are rotting away along with your core joints with severe attachment issues.\n",
      "\n",
      "SW: I guess that is what I was to most insulting commenters. When I proudly claim \"I am, without a doubt, the\n"
     ]
    }
   ],
   "source": [
    "result = gpt2.generate_conditional(raw_text=\"Can you tell me something about music?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erXh3GvcP1Ps"
   },
   "outputs": [],
   "source": [
    "class Who:\n",
    "  \"\"\"A class defining the conversation parties: me, he\"\"\"\n",
    "  def __init__(self):\n",
    "    self.prefixes = []\n",
    "\n",
    "  def matches(self,phrase):\n",
    "    for prefix in self.prefixes:\n",
    "      if phrase.startswith(prefix):\n",
    "        #print(f\"{phrase} starts with {prefix}\")\n",
    "        return True\n",
    "      \n",
    "    #print(f\"{phrase} does not start with {self.prefixes}\")\n",
    "    return False\n",
    "\n",
    "  def get_random_prefix(self):\n",
    "    return self.prefixes[0]\n",
    "  \n",
    "class Me(Who):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.prefixes = [\"I said: \\\"\"]\n",
    "   \n",
    "  \n",
    "class You(Who):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.prefixes = [\"You said: \\\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WjS58QZlrVKN"
   },
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "  \n",
    "  def __init__(self, prior = None):\n",
    "    if prior is None:\n",
    "      prior=\"\"\"\n",
    "      You said: \"Nice to meet you. What's your name?\"\n",
    "      I said: \"My name is Pete.\"\n",
    "      You said: \"That's an interesting name. How old are you?\"\n",
    "      I said: \"I'm 40 years old.\"\n",
    "      You said: \"Can you tell me something about yourself?\"\n",
    "      I said: \"Ofcourse! I like playing video games and eating cake. \"\n",
    "      You said: \"I like sweet stuff too. What are your plans for tomorrow?\"\n",
    "      \"\"\"\n",
    "    self.suggestion = None\n",
    "    \n",
    "    self.me = Me()\n",
    "    self.you = You()\n",
    "    self.parties  = [ self.me, self.you ]\n",
    "    \n",
    "    self.conversation = []\n",
    "    \n",
    "    lines = prior.split(\"\\n\")\n",
    "    for line in lines:\n",
    "      line = line.strip()\n",
    "      if len(line)!=0:\n",
    "        party = None\n",
    "        for party in self.parties:\n",
    "          if party.matches(line):\n",
    "            break\n",
    "        if party is None:\n",
    "          raise Exception(f\"Unknown party: {line}\")\n",
    "                \n",
    "        self.conversation.append((party,line))\n",
    "    self.get_suggestion()\n",
    "    \n",
    "  \n",
    "  def get_prior(self):\n",
    "    conv = \"\"\n",
    "    for (party, line) in self.conversation:\n",
    "      conv+=line+\"\\n\"\n",
    "    return conv\n",
    "  \n",
    "  def get_suggestion(self):\n",
    "    who, last_line = self.conversation[-1]\n",
    "\n",
    "    party_index = self.parties.index(who)\n",
    "    next_party = self.parties[(party_index+1) % len(self.parties)]\n",
    "      \n",
    "    conv = self.get_prior()\n",
    "    conv += next_party.get_random_prefix()\n",
    "    answer = self.get_answer(next_party, conv)\n",
    "\n",
    "    if not next_party.matches(answer):\n",
    "      prefix = next_party.get_random_prefix()\n",
    "      answer = prefix + answer\n",
    "    \n",
    "    self.suggestion = (next_party, answer)\n",
    "  \n",
    "  def next(self, party = None, answer = \"\"):\n",
    "    \"\"\"Continue the conversation\n",
    "    :param party: None -> use the current party which is currently in turn\n",
    "    :param answer: None -> use the suggestion, specify a text to override the \n",
    "           suggestion\n",
    "    \n",
    "    \"\"\"\n",
    "    suggested_party, suggested_answer = self.suggestion\n",
    "    if party is None:\n",
    "      party = suggested_party\n",
    "    \n",
    "    if answer == \"\":\n",
    "      answer = suggested_answer\n",
    "      \n",
    "    if not party.matches(answer):\n",
    "      prefix = party.get_random_prefix()\n",
    "      answer = prefix + answer\n",
    "    \n",
    "    answer = answer.strip()\n",
    "    if answer[-1] != \"\\\"\":\n",
    "      # add the closing \"\n",
    "      answer += \"\\\"\"\n",
    "      \n",
    "    self.conversation.append((party, answer))    \n",
    "    self.get_suggestion()\n",
    "    \n",
    "  def retry(self):\n",
    "    self.get_suggestion()\n",
    "        \n",
    "  def get_answer(self, party, conv):\n",
    "    answer = gpt2.generate_conditional(raw_text=conv)\n",
    "    lines = answer.split(\"\\n\")\n",
    "    line = \"\"\n",
    "    for line in lines:\n",
    "      if line !=\"\":\n",
    "        break\n",
    "      \n",
    "    if line!=\"\":\n",
    "      return line\n",
    "    \n",
    "    return \"\"\n",
    "      \n",
    "  def show(self):\n",
    "    conv = \"\"\n",
    "    for (party, line) in self.conversation:\n",
    "      conv+=line+\"\\n\"\n",
    "    print(conv)\n",
    "    if self.suggestion is not None:\n",
    "      party, answer  = self.suggestion\n",
    "      print(\"--> \"+answer)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GYnMZDPgr7A"
   },
   "outputs": [],
   "source": [
    "# 'You' is the bot\n",
    "prior_bot_annie = \"\"\"\n",
    "I said: hello. my name is annie. what's your name?\n",
    "You said: hi. my name is bottie. \n",
    "I said: so nice to meet you!\n",
    "You said: you too!\n",
    "I said: how old are you?\n",
    "You said: i'm 20 years old.\n",
    "I said: nice! i'm 21 years old. i am in college right now.\n",
    "You said: what's your favorite class?\n",
    "\"\"\"\n",
    "\n",
    "# Alex is the bot\n",
    "prior_alex = \"\"\"\n",
    "I said: hi Annie. how are you? \n",
    "Annie said: Alex, i'm so sorry i could not come get lunch with you and lauren today. hope you had a good thanksgiving break though. also, are you back for christmas?\n",
    "\"\"\"\n",
    "\n",
    "c = Conversation(prior_alex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "taqx6zfNVbru",
    "outputId": "b8edb905-d29d-4892-c584-57bbaf4d5552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annie said: hello. my name is annie. what's your name?\n",
      "Bottie said: hi. my name is bottie.\n",
      "Annie said: so nice to meet you!\n",
      "Bottie said: you too!\n",
      "Annie said: how old are you?\n",
      "Bottie said: i'm 20 years old.\n",
      "Annie said: nice! i'm 21 years old. i am in college right now.\n",
      "Bottie said: what's your favorite class?\n",
      "\n",
      "--> I said: \"Act I.\" I said that when i was 19 years old. But i never asked because i didn't want to hurt my voice, and i have carried my ass in rehearsals since then. I love to act on stage.\n"
     ]
    }
   ],
   "source": [
    "# show the conversation and the suggestion by the ai\n",
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dUICPSgdfcp_"
   },
   "outputs": [],
   "source": [
    "# \"I said\" -> answer by the AI\n",
    "# if the answer of the AI is garbage then call c.retry() \n",
    "c.retry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5519441483542119\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "-SvC-10-WTKH",
    "outputId": "dc5b8415-b8d8-417c-b02c-d2408e54336c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annie said: hello. my name is annie. what's your name?\n",
      "Bottie said: hi. my name is bottie.\n",
      "Annie said: so nice to meet you!\n",
      "Bottie said: you too!\n",
      "Annie said: how old are you?\n",
      "Bottie said: i'm 20 years old.\n",
      "Annie said: nice! i'm 21 years old. i am in college right now.\n",
      "Bottie said: what's your favorite class?\n",
      "\n",
      "--> I said: \"biology\"\n"
     ]
    }
   ],
   "source": [
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNsuyeKyeNeq"
   },
   "outputs": [],
   "source": [
    "# Accept the suggested answer by the AI\n",
    "c.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJ8F3HbyUDZh"
   },
   "outputs": [],
   "source": [
    "\n",
    "# now its your turn\n",
    "c.next(c.you, \"My favorite pizza is the calzone with lots of cheese.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "1Ts3yigFeQ-8",
    "outputId": "d135363b-66e6-406d-e055-386542c1b9ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You said: \"Nice to meet you. What's your name?\"\n",
      "I said: \"My name is Pete.\"\n",
      "You said: \"That's an interesting name. How old are you?\"\n",
      "I said: \"I'm 40 years old.\"\n",
      "You said: \"Can you tell me something about yourself?\"\n",
      "I said: \"Ofcourse! I like playing video games and eating cake. \"\n",
      "You said: \"I like sweet stuff too. What are your plans for tomorrow?\"\n",
      "I said: \"Nothing, and I don't know what to do with my life! \"\n",
      "\n",
      "--> You said: \"Do you like following the advice of experts? Or do you have all your own ideas?\"\n"
     ]
    }
   ],
   "source": [
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bR5UP1WQXCi0"
   },
   "outputs": [],
   "source": [
    "c.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUiQqYdEhmwe"
   },
   "outputs": [],
   "source": [
    "# now its your turn\n",
    "c.next(c.you, \"You eat pizza often?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "snnqjfilc1Ai",
    "outputId": "d4447020-8cb1-44be-abe1-83390cef3a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You said: \"Nice to meet you. What's your name?\"\n",
      "I said: \"My name is Pete.\"\n",
      "You said: \"That's an interesting name. How old are you?\"\n",
      "I said: \"I'm 40 years old.\"\n",
      "You said: \"Can you tell me something about yourself?\"\n",
      "I said: \"Ofcourse! I like playing video games and eating cake. \"\n",
      "You said: \"I like sweet stuff too. What are your plans for tomorrow?\"\n",
      "I said: \"I will participate in playing video games and try to eaten pizza, I'm going to want some apples 'cause I kiss big apples...ill be eating them.\"\n",
      "You said: \"My favorite pizza is the calzone with lots of cheese.\"\n",
      "I said: \"Sounds good.\"\n",
      "You said: \"You eat pizza often?\"\n",
      "\n",
      "--> I said: \"Yes, I do. I would like to eat more often...and rock my rocking armor.\"\n"
     ]
    }
   ],
   "source": [
    "# show the conversation and the reply of the ai\n",
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8R6da2YMS6fD"
   },
   "outputs": [],
   "source": [
    "# accept ai answer\n",
    "c.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPJZ_aJxltCm"
   },
   "outputs": [],
   "source": [
    "# our reply\n",
    "c.next(c.you, \"Pizza is not to good for your health though.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "e66v5ew6sQW4",
    "outputId": "921d7494-68b1-4e19-cf89-a1904349aee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You said: \"Nice to meet you. What's your name?\"\n",
      "I said: \"My name is Pete.\"\n",
      "You said: \"That's an interesting name. How old are you?\"\n",
      "I said: \"I'm 40 years old.\"\n",
      "You said: \"Can you tell me something about yourself?\"\n",
      "I said: \"Ofcourse! I like playing video games and eating cake. \"\n",
      "You said: \"I like sweet stuff too. What are your plans for tomorrow?\"\n",
      "I said: \"I will participate in playing video games and try to eaten pizza, I'm going to want some apples 'cause I kiss big apples...ill be eating them.\"\n",
      "You said: \"My favorite pizza is the calzone with lots of cheese.\"\n",
      "I said: \"Sounds good.\"\n",
      "You said: \"You eat pizza often?\"\n",
      "I said: \"Yes, I do. I would like to eat more often...and rock my rocking armor.\"\n",
      "You said: \"Pizza is not to good for your health though.\"\n",
      "\n",
      "--> I said: \"Oh yeah. I do get so tired from it.\"\n"
     ]
    }
   ],
   "source": [
    "# show the conv. and reply of the ai\n",
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u2epsBUCPtcc"
   },
   "outputs": [],
   "source": [
    "# end the session\n",
    "gpt2.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of GPT-2 conversation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
