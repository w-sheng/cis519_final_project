{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of GPT-2 conversation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLXW02eIYpcB",
        "colab_type": "text"
      },
      "source": [
        "clone and cd into repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICYu3w9hIJkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first download the gpt-2 code\n",
        "!git clone https://github.com/nshepperd/gpt-2.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBvmbwX7ePev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eEIs3ApZUVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd gpt-2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtn1qZPgZLb0",
        "colab_type": "text"
      },
      "source": [
        "install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "434oOx0bZH6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1hrgeKFYsuE",
        "colab_type": "text"
      },
      "source": [
        "download the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQUZAa9cIcTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download the pretrained model\n",
        "!python download_model.py 345M"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oJPQtdLbbeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrV4qjoWc8ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUw5FEn-fIqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/gpt-2/src\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgs6ekD1fItO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import fire\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import model, sample, encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Hqb_vDxfMza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import generate_unconditional_samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRMB0HAjhC2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import interactive_conditional_samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvkAIuq5fb3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GPT2:\n",
        "\n",
        "  \n",
        "  # extracted from the source code to generate some text based on a prior\n",
        "  def __init__(\n",
        "      self,\n",
        "      model_name='345M',\n",
        "      seed=None,\n",
        "      nsamples=1,\n",
        "      batch_size=1,\n",
        "      length=None,\n",
        "      temperature=1,\n",
        "      top_k=0,\n",
        "      raw_text=\"\",\n",
        "  ):\n",
        "      \"\"\"\n",
        "      Interactively run the model\n",
        "      :model_name=117M : String, which model to use\n",
        "      :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
        "       results\n",
        "      :nsamples=1 : Number of samples to return total\n",
        "      :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
        "      :length=None : Number of tokens in generated text, if None (default), is\n",
        "       determined by model hyperparameters\n",
        "      :temperature=1 : Float value controlling randomness in boltzmann\n",
        "       distribution. Lower temperature results in less random completions. As the\n",
        "       temperature approaches zero, the model will become deterministic and\n",
        "       repetitive. Higher temperature results in more random completions.\n",
        "      :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
        "       considered for each step (token), resulting in deterministic completions,\n",
        "       while 40 means 40 words are considered at each step. 0 (default) is a\n",
        "       special setting meaning no restrictions. 40 generally is a good value.\n",
        "      \"\"\"\n",
        "      if batch_size is None:\n",
        "          batch_size = 1\n",
        "      assert nsamples % batch_size == 0\n",
        "\n",
        "      self.nsamples = nsamples\n",
        "      self.batch_size = batch_size\n",
        "      \n",
        "      self.enc = encoder.get_encoder(model_name)\n",
        "      hparams = model.default_hparams()\n",
        "      with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
        "          hparams.override_from_dict(json.load(f))\n",
        "\n",
        "      if length is None:\n",
        "          length = hparams.n_ctx // 2\n",
        "      elif length > hparams.n_ctx:\n",
        "          raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "      self.sess = tf.Session(graph=tf.Graph())\n",
        "      self.sess.__enter__()\n",
        "      \n",
        "      self.context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "      np.random.seed(seed)\n",
        "      tf.set_random_seed(seed)\n",
        "      self.output = sample.sample_sequence(\n",
        "          hparams=hparams, length=length,\n",
        "          context=self.context,\n",
        "          batch_size=batch_size,\n",
        "          temperature=temperature, top_k=top_k\n",
        "      )\n",
        "\n",
        "      saver = tf.train.Saver()\n",
        "      self.ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name))\n",
        "      saver.restore(self.sess, self.ckpt)\n",
        "\n",
        "  def close(self):\n",
        "    self.sess.close()\n",
        "  \n",
        "  def generate_conditional(self,raw_text):\n",
        "      context_tokens = self.enc.encode(raw_text)\n",
        "      generated = 0\n",
        "      for _ in range(self.nsamples // self.batch_size):\n",
        "          out = self.sess.run(self.output, feed_dict={\n",
        "              self.context: [context_tokens for _ in range(self.batch_size)]\n",
        "          })[:, len(context_tokens):]\n",
        "          for i in range(self.batch_size):\n",
        "              generated += 1\n",
        "              text = self.enc.decode(out[i])\n",
        "              return text\n",
        "              #print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "              #print(text)\n",
        "      #print(\"=\" * 80)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoJbf02UOFed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2 = GPT2()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAdyhE0JgFV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = gpt2.generate_conditional(raw_text=\"Can you tell me something about music?\")\n",
        "\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erXh3GvcP1Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Who:\n",
        "  \"\"\"A class defining the conversation parties: me, he\"\"\"\n",
        "  def __init__(self):\n",
        "    self.prefixes = []\n",
        "\n",
        "  def matches(self,phrase):\n",
        "    for prefix in self.prefixes:\n",
        "      if phrase.startswith(prefix):\n",
        "        #print(f\"{phrase} starts with {prefix}\")\n",
        "        return True\n",
        "      \n",
        "    #print(f\"{phrase} does not start with {self.prefixes}\")\n",
        "    return False\n",
        "\n",
        "  def get_random_prefix(self):\n",
        "    return self.prefixes[0]\n",
        "  \n",
        "class Me(Who):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.prefixes = [\"I said: \\\"\"]\n",
        "   \n",
        "  \n",
        "class You(Who):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.prefixes = [\"You said: \\\"\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjS58QZlrVKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Conversation:\n",
        "  \n",
        "  def __init__(self, prior = None):\n",
        "    if prior is None:\n",
        "      prior=\"\"\"\n",
        "      You said: \"Nice to meet you. What's your name?\"\n",
        "      I said: \"My name is Pete.\"\n",
        "      You said: \"That's an interesting name. How old are you?\"\n",
        "      I said: \"I'm 40 years old.\"\n",
        "      You said: \"Can you tell me something about yourself?\"\n",
        "      I said: \"Ofcourse! I like playing video games and eating cake. \"\n",
        "      You said: \"I like sweet stuff too. What are your plans for tomorrow?\"\n",
        "      \"\"\"\n",
        "    self.suggestion = None\n",
        "    \n",
        "    self.me = Me()\n",
        "    self.you = You()\n",
        "    self.parties  = [ self.me, self.you ]\n",
        "    \n",
        "    self.conversation = []\n",
        "    \n",
        "    lines = prior.split(\"\\n\")\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if len(line)!=0:\n",
        "        party = None\n",
        "        for party in self.parties:\n",
        "          if party.matches(line):\n",
        "            break\n",
        "        if party is None:\n",
        "          raise Exception(f\"Unknown party: {line}\")\n",
        "                \n",
        "        self.conversation.append((party,line))\n",
        "    self.get_suggestion()\n",
        "    \n",
        "  \n",
        "  def get_prior(self):\n",
        "    conv = \"\"\n",
        "    for (party, line) in self.conversation:\n",
        "      conv+=line+\"\\n\"\n",
        "    return conv\n",
        "  \n",
        "  def get_suggestion(self):\n",
        "    who, last_line = self.conversation[-1]\n",
        "\n",
        "    party_index = self.parties.index(who)\n",
        "    next_party = self.parties[(party_index+1) % len(self.parties)]\n",
        "      \n",
        "    conv = self.get_prior()\n",
        "    conv += next_party.get_random_prefix()\n",
        "    answer = self.get_answer(next_party, conv)\n",
        "\n",
        "    if not next_party.matches(answer):\n",
        "      prefix = next_party.get_random_prefix()\n",
        "      answer = prefix + answer\n",
        "    \n",
        "    self.suggestion = (next_party, answer)\n",
        "  \n",
        "  def next(self, party = None, answer = \"\"):\n",
        "    \"\"\"Continue the conversation\n",
        "    :param party: None -> use the current party which is currently in turn\n",
        "    :param answer: None -> use the suggestion, specify a text to override the \n",
        "           suggestion\n",
        "    \n",
        "    \"\"\"\n",
        "    suggested_party, suggested_answer = self.suggestion\n",
        "    if party is None:\n",
        "      party = suggested_party\n",
        "    \n",
        "    if answer == \"\":\n",
        "      answer = suggested_answer\n",
        "      \n",
        "    if not party.matches(answer):\n",
        "      prefix = party.get_random_prefix()\n",
        "      answer = prefix + answer\n",
        "    \n",
        "    answer = answer.strip()\n",
        "    if answer[-1] != \"\\\"\":\n",
        "      # add the closing \"\n",
        "      answer += \"\\\"\"\n",
        "      \n",
        "    self.conversation.append((party, answer))    \n",
        "    self.get_suggestion()\n",
        "    \n",
        "  def retry(self):\n",
        "    self.get_suggestion()\n",
        "        \n",
        "  def get_answer(self, party, conv):\n",
        "    answer = gpt2.generate_conditional(raw_text=conv)\n",
        "    lines = answer.split(\"\\n\")\n",
        "    line = \"\"\n",
        "    for line in lines:\n",
        "      if line !=\"\":\n",
        "        break\n",
        "      \n",
        "    if line!=\"\":\n",
        "      return line\n",
        "    \n",
        "    return \"\"\n",
        "      \n",
        "  def show(self):\n",
        "    conv = \"\"\n",
        "    for (party, line) in self.conversation:\n",
        "      conv+=line+\"\\n\"\n",
        "    print(conv)\n",
        "    if self.suggestion is not None:\n",
        "      party, answer  = self.suggestion\n",
        "      print(\"--> \"+answer)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GYnMZDPgr7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = Conversation(\"\"\"\n",
        "Annie said: hello. my name is annie. what's your name?\n",
        "Bottie said: hi. my name is bottie. \n",
        "Annie said: so nice to meet you!\n",
        "Bottie said: you too!\n",
        "Annie said: how old are you?\n",
        "Bottie said: i'm 20 years old.\n",
        "Annie said: nice! i'm 21 years old. i am in college right now.\n",
        "Bottie said: what's your favorite class?\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taqx6zfNVbru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show the conversation and the suggestion by the ai\n",
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUICPSgdfcp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"I said\" -> answer by the AI\n",
        "# if the answer of the AI is garbage then call c.retry() \n",
        "c.retry()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SvC-10-WTKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNsuyeKyeNeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Accept the suggested answer by the AI\n",
        "c.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ8F3HbyUDZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# now its your turn\n",
        "c.next(c.you, \"My favorite pizza is the calzone with lots of cheese.\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ts3yigFeQ-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR5UP1WQXCi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUiQqYdEhmwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now its your turn\n",
        "c.next(c.you, \"You eat pizza often?\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snnqjfilc1Ai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show the conversation and the reply of the ai\n",
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R6da2YMS6fD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accept ai answer\n",
        "c.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPJZ_aJxltCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# our reply\n",
        "c.next(c.you, \"Pizza is not to good for your health though.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e66v5ew6sQW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show the conv. and reply of the ai\n",
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2epsBUCPtcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# end the session\n",
        "gpt2.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}